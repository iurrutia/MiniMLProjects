{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "\n",
    "- Scrape data from Reddit Stadia Subreddit\n",
    "- Train model on reddit data set\n",
    "- Train model (sentiment analysis)\n",
    "- Deploy model on Stadia Subreddit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up virtual env:\n",
    "- Navigate to the right folder, then create a virtual env:\n",
    "\n",
    "` virtualenv -p python3 env `\n",
    "` source env/bin/activate `\n",
    "(now all python commands/packages are in the virtual environment)\n",
    "\n",
    "(Note: if openning a new terminal, you just have to do the `source env/bin/activate` step)\n",
    "- Install praw in the terminal:\n",
    "\n",
    "`pip install praw`\n",
    "\n",
    "Then here we install it: `import praw`\n",
    "\n",
    "(using this with https://towardsdatascience.com/scraping-reddit-data-1c0af3040768)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from Stadia Subreddit (using https://towardsdatascience.com/scraping-reddit-data-1c0af3040768)\n",
    "\n",
    "import praw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a praw instance, I use the following id:\n",
    "\n",
    "- NAME (user_agent): Reddit_Stadia_praw\n",
    "- personal use script (client_id): ehIwK_qCB7rZ9A\n",
    "- Secret: VpDCWnTFYMDs10b6F3rt3XysD1s\n",
    "\n",
    "`reddit = praw.Reddit(client_id='my_client_id', client_secret='my_client_secret', user_agent='my_user_agent')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='ehIwK_qCB7rZ9A', client_secret='VpDCWnTFYMDs10b6F3rt3XysD1s', user_agent='Reddit_Stadia_praw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of scraping data with praw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Stadia Team presents “I Made This” -- A Play it Forward Community Event\n",
      "Buddy Thread 2.0 - Find friends, group up, gift or find a pass, share usernames, post links etc\n",
      "Yep, definitely\n",
      "Stadia - London Pop Up. Managed to get this big blanket.\n",
      "PSA: Want a game on Stadia? Tell the developers and publishers.\n",
      "Living my best life\n",
      "The store now has YouTube video trailers!\n",
      "Super cool Stadia event in Hollywood! The game streaming was as smooth as the free hot cocoa. ☕\n",
      "Digital Foundry - Metro Exodus on Stadia Tested! Better than Xbox One X?\n",
      "Breakpoint reconfirmed for end of 2019 by twitter support\n"
     ]
    }
   ],
   "source": [
    "# test:\n",
    "# get 10 hot posts from the Stadia subreddit\n",
    "hot_posts = reddit.subreddit('Stadia').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score      id  \\\n",
      "0  [D] Machine Learning - WAYR (What Are You Read...     20  e4nmyk   \n",
      "1  [D] What was your favorite paper of 2019 and why?    109  e8the3   \n",
      "2  [R] StyleGAN2: Analyzing and Improving the Ima...    112  e9md4j   \n",
      "3  [D] What do you think were the most important ...      8  e9rwj9   \n",
      "4  [N] Kaggle Deep Fake detection: 470Gb of video...    609  e9apif   \n",
      "5  [D] Things to predict from human skeleton/post...      5  e9rw8f   \n",
      "6  [N] Dragonfire v1.1.0: DeepPavlov SQuAD BERT I...      2  e9t0da   \n",
      "7  [D] What is the definition of one-stage vs. tw...      5  e9nm6b   \n",
      "8   [D] Open Exposition Problems in Machine Learning      3  e9p43w   \n",
      "9  [Discussion] My boss is convinced you can do a...      0  e9taa3   \n",
      "\n",
      "         subreddit                                                url  \\\n",
      "0  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "1  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "2  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "3  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "4  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "5  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "6  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "7  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "8  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "9  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "\n",
      "   num_comments                                               body  \\\n",
      "0            14  This is a place to share machine learning rese...   \n",
      "1            44  As the year, and [decade](https://www.reddit.c...   \n",
      "2            12  **Abstract**\\n\\nThe style-based GAN architectu...   \n",
      "3             3  2019 has been yet another properous year for t...   \n",
      "4           113  [https://www.kaggle.com/c/deepfake-detection-c...   \n",
      "5             1  I'm in charge of designing a new lab for a cou...   \n",
      "6             0  We are happy to announce the Dragonfire v1.0.0...   \n",
      "7             1  I frequently see the distinction between one- ...   \n",
      "8             0  In his paper \"A Beginner's Guide to Forcing,\" ...   \n",
      "9             0   Where do I even begin this rant?\\n\\nI  am a m...   \n",
      "\n",
      "        created  \n",
      "0  1.575263e+09  \n",
      "1  1.576027e+09  \n",
      "2  1.576179e+09  \n",
      "3  1.576206e+09  \n",
      "4  1.576117e+09  \n",
      "5  1.576206e+09  \n",
      "6  1.576211e+09  \n",
      "7  1.576186e+09  \n",
      "8  1.576194e+09  \n",
      "9  1.576212e+09  \n",
      "**Abstract**\n",
      "\n",
      "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.\n",
      "\n",
      "Arxiv: [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958) \n",
      "\n",
      "PDF: [https://arxiv.org/pdf/1912.04958.pdf](https://arxiv.org/pdf/1912.04958.pdf) \n",
      "\n",
      "Youtube: [https://www.youtube.com/watch?v=c-NJtV9Jvp0](https://www.youtube.com/watch?v=c-NJtV9Jvp0) \n",
      "\n",
      "Code: [https://github.com/NVlabs/stylegan2](https://github.com/NVlabs/stylegan2) \n",
      " Source url: https://www.reddit.com/r/MachineLearning/comments/e9md4j/r_stylegan2_analyzing_and_improving_the_image/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "for post in ml_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "print(posts)\n",
    "print(posts.iloc[2,6], '\\n', \"Source url:\", posts.iloc[2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
      "--------\n",
      "+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
      "--------\n",
      "+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
      "--------\n",
      "+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
      "--------\n",
      "+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ANews)\n",
      "--------\n",
      "***[@slashML on Twitter](https://twitter.com/slashML)***\n",
      "--------\n",
      "***[Chat with us on Slack](https://join.slack.com/t/rml-talk/shared_invite/enQtNjkyMzI3NjA2NTY2LWY0ZmRjZjNhYjI5NzYwM2Y0YzZhZWNiODQ3ZGFjYmI2NTU3YjE1ZDU5MzM2ZTQ4ZGJmOTFmNWVkMzFiMzVhYjg)***\n",
      "--------\n",
      "**Beginners:**\n",
      "--------\n",
      "Please have a look at [our FAQ and Link-Collection](http://www.reddit.com/r/MachineLearning/wiki/index)\n",
      "\n",
      "[Metacademy](http://www.metacademy.org) is a great resource which compiles lesson plans on popular machine learning topics.\n",
      "\n",
      "For Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/\n",
      "\n",
      "For career related questions, visit /r/cscareerquestions/\n",
      "\n",
      "--------\n",
      "\n",
      "[Advanced Courses](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses?st=isz2lqdk&sh=56c58cd6)\n",
      "\n",
      "--------\n",
      "**AMAs:**\n",
      "\n",
      "[Pluribus Poker AI Team 7/19/2019](https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/)\n",
      "\n",
      "[DeepMind AlphaStar team (1/24//2019)](https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/)\n",
      "\n",
      "[Libratus Poker AI Team (12/18/2017)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/7jn12v/ama_we_are_noam_brown_and_professor_tuomas/)\n",
      "\n",
      "[DeepMind AlphaGo Team (10/19/2017)](https://www.reddit.com/r/MachineLearning/comments/76xjb5/ama_we_are_david_silver_and_julian_schrittwieser/)\n",
      "\n",
      "[Google Brain Team (9/17/2017)](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/)\n",
      "\n",
      "[Google Brain Team (8/11/2016)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/)\n",
      "\n",
      "[The MalariaSpot Team (2/6/2016)](https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/)\n",
      "\n",
      "[OpenAI Research Team (1/9/2016)](http://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/)\n",
      "\n",
      "[Nando de Freitas (12/26/2015)](http://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/)\n",
      "\n",
      "[Andrew Ng and Adam Coates (4/15/2015)](http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/)\n",
      "\n",
      "[Jürgen Schmidhuber (3/4/2015)](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)\n",
      "\n",
      "[Geoffrey Hinton (11/10/2014)]\n",
      "(http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/)\n",
      "\n",
      "[Michael Jordan (9/10/2014)](http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)\n",
      "\n",
      "[Yann LeCun (5/15/2014)](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/)\n",
      "\n",
      "[Yoshua Bengio (2/27/2014)](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)\n",
      "\n",
      "--------\n",
      "Related Subreddit :\n",
      "\n",
      "* [LearnMachineLearning](http://www.reddit.com/r/LearnMachineLearning)\n",
      "\n",
      "* [Statistics](http://www.reddit.com/r/statistics)\n",
      "\n",
      "* [Computer Vision](http://www.reddit.com/r/computervision)\n",
      "\n",
      "* [Compressive Sensing](http://www.reddit.com/r/CompressiveSensing/)\n",
      "\n",
      "* [NLP] (http://www.reddit.com/r/LanguageTechnology)\n",
      "\n",
      "* [ML Questions] (http://www.reddit.com/r/MLQuestions)\n",
      "\n",
      "* /r/MLjobs and /r/BigDataJobs\n",
      "\n",
      "* /r/datacleaning\n",
      "\n",
      "* /r/DataScience\n",
      "\n",
      "* /r/scientificresearch\n",
      "\n",
      "* /r/artificial\n"
     ]
    }
   ],
   "source": [
    "# Getting info about the subreddit:\n",
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "\n",
    "print(ml_subreddit.description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
